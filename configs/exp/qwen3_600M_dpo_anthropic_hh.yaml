run:
  name: "qwen3_600M_dpo_anthropic_hh"
  mode: "dpo"

model:
  type: "hf_causal"
  id: "outputs/qwen3_600M_sft_mixed_chat_12ksteps_2048seq"
  dtype: "bfloat16"
  gradient_checkpointing: false

tokenizer:
  type: "hf"
  id: "outputs/qwen3_600M_sft_mixed_chat_12ksteps_2048seq"

data:
  type: "dpo"
  source: "hf"
  train_path: "Anthropic/hh-rlhf"
  eval_path: "Anthropic/hh-rlhf"
  max_seq_len: 2048
  num_proc: 12

training:
  # Batch settings optimized for 5070 Ti (16GB VRAM)
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2

  # Dataloader optimization for faster throughput
  dataloader_num_workers: 12
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4

  # Training schedule
  max_steps: 2000
  save_steps: 500
  eval_steps: 500

  # Learning rate - FIXED (was 10x too low)
  learning_rate: 5.0e-6
  warmup_steps: 100
  lr_scheduler_type: "cosine"
  weight_decay: 0.01
  max_grad_norm: 1.0

  # DPO-specific settings
  beta: 0.1
  max_length: 2048
  max_prompt_length: 512

  logging_steps: 10
  logging_first_step: true
  bf16: true
  torch_compile: false

wandb:
  tags: ["qwen3-0.6b", "dpo", "anthropic-hh", "from-sft-mixed", "optimized"]
