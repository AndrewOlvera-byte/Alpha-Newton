run:
  name: "qwen3_600M_dpo_test"
  mode: "dpo"

model:
  type: "hf_causal"
  id: "Qwen/Qwen2.5-0.5B-Instruct"
  dtype: "bfloat16"
  gradient_checkpointing: true

tokenizer:
  type: "hf"
  id: "Qwen/Qwen2.5-0.5B-Instruct"

data:
  type: "dpo"
  source: "hf"
  train_path: "Anthropic/hh-rlhf"
  eval_path: "Anthropic/hh-rlhf"
  max_seq_len: 1024
  num_proc: 12

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 2

  dataloader_num_workers: 4
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2

  max_steps: 50
  save_steps: 25
  eval_steps: 25
  learning_rate: 5.0e-7
  warmup_steps: 10
  beta: 0.1

  logging_steps: 5
  logging_first_step: true

wandb:
  tags: ["qwen2.5-0.5b", "dpo", "anthropic-hh", "test-run"]
