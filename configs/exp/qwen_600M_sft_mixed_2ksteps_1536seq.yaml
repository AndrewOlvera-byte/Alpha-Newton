run:
  name: "qwen3_600M_sft_mixed_2ksteps_1536seq"
  mode: "sft"

model:
  type: "hf_causal"
  id: "Qwen/Qwen3-0.6B-Base"
  dtype: "bfloat16"
  gradient_checkpointing: false

tokenizer:
  type: "hf"
  id: "Qwen/Qwen3-0.6B-Base"

data:
  type: "sft_mixed"
  datasets_config:
    - path: "teknium/OpenHermes-2.5"
      weight: 0.7
    - path: "glaiveai/glaive-function-calling-v2"
      weight: 0.3
  max_seq_len: 1536
  num_proc: 12
  packing: true

training:
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2

  max_steps: 2000
  save_steps: 500
  eval_steps: 500
  learning_rate: 2.0e-5
  warmup_steps: 200

  logging_steps: 50
  logging_first_step: true
  bf16: true
  torch_compile: true

wandb:
  tags: ["qwen3-0.6b", "sft", "mixed", "openhermes+glaive", "2ksteps", "1536seq"]
